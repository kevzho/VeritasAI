{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ed127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#setup\n",
    "#code\n",
    "name = 'NVDA'\n",
    "ticker = yf.Ticker(name) #importÂ data\n",
    "aapl_df = ticker.history(period=\"5y\") #get data from 5 year period in dataframe\n",
    "aapl_df.drop(['High','Close','Volume','Dividends','Stock Splits'], axis=1, inplace=True) #only have two columns, not seven\n",
    "open = np.empty(shape = (1259), dtype = float)\n",
    "close = np.empty(shape = (1259), dtype = float)\n",
    "\n",
    "open=aapl_df[['Open']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b9b978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.1986341  10.34016909 10.35661174] 10.340915971627338\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((1260,3))\n",
    "Y = [0]*1260\n",
    "#sliding window with a fixed window of 3 days, x is the 3 days, y is the next day outside the window (our prediction)\n",
    "for i in range(1250):\n",
    "  X[i] = [open[i][0],open[i+1][0],open[i+2][0]]\n",
    "  Y[i] = open[i+3][0]\n",
    "\n",
    "#gives you 3 consecutive days as input and the next day as output\n",
    "print(X[0], Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "916626ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "#shuffle so that we won't be biased and we want the general pattern of the last 5 years\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77aaba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Example: Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Ensure Y tensors are 2D (N, 1)\n",
    "if Y_train_tensor.ndim == 1:\n",
    "    Y_train_tensor = Y_train_tensor.unsqueeze(1)\n",
    "if Y_test_tensor.ndim == 1:\n",
    "    Y_test_tensor = Y_test_tensor.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2bc8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi layer perceptron (MLP) used for regression\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(MLPRegressor, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Output 1 value for regression\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate model\n",
    "input_dim = X_train.shape[1]\n",
    "model = MLPRegressor(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fde54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4475.9487\n",
      "Epoch 10, Loss: 1419.3434\n",
      "Epoch 20, Loss: 85.4305\n",
      "Epoch 30, Loss: 89.5767\n",
      "Epoch 40, Loss: 32.0888\n",
      "Epoch 50, Loss: 24.1430\n",
      "Epoch 60, Loss: 8.7994\n",
      "Epoch 70, Loss: 10.5018\n",
      "Epoch 80, Loss: 9.1439\n",
      "Epoch 90, Loss: 8.7419\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, Y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "#Convergence : Model loss has stopped decreasing, reaching a plateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106605b",
   "metadata": {},
   "source": [
    "To decide what activation functions to use in our neural network (MLP Regressor), just do trial and error - here are some we can choose from:\n",
    "- ReLU\n",
    "- Tanh\n",
    "- Softmax\n",
    "- Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb6c284d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 6.9433\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor).squeeze().numpy()\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Test MSE: {mse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
